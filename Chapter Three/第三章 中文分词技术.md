# 第三章 中文分词技术
## 中文分词简介
自中文自动分词被提及以来，经历将近30年的探索，提出了很多方法，可主要归纳为“规则分词”、“统计分词”和“混合分词(规则+统计)”这三个主要流派。规则分词是最早兴起的方法，主要是通过人工设立词库，按照一定方式进行匹配切分，其实现简单高效，但对新词很难进行处理。随后统计机器学习技术的兴起，应用于分词任务上后，就有了统计分词，能够较好的应对新词发现等特殊场景。然而实践中，单纯的统计分词也有缺陷，那就是太多于依赖语料的质量，因此实践中多是采用这两种方法的结合，即混合分词。
## 规则分词
基于规则的分词是一种机械分词方法，主用通过维护词典，在切分语句时，将语句的每个字符串与词表中的词进行逐一匹配，找到则切分，否则不予切分。按照匹配切分方式，主要有正向最大匹配法、逆向最大匹配法以及双向最大匹配法三种方式。
### 正向最大匹配法
正向最大匹配(Maximum Match Method, MM法)的基本思想：假定分词词典中的最长词有i个汉字字符，则用被处理文档的当前字符串的前i个字作为匹配字段，查找字典。若字典中存在这样的一个i字词，则匹配成功，匹配字段被作为一个词切分出来。如果词典中找不到这样的一个i字词，则匹配失败，将匹配字段中的最后一个字去掉，对剩下的字串重新进行匹配处理。如此进行下去，直到匹配成功，即切分出一个词或剩余字串的长度为零为止。这样就完成了一轮匹配，然后取下一个i字字串进行匹配处理，直到文档被扫描为止。其算法描述如下：  
1.从左向右取待切分汉语句的m个字符作为匹配字段，m为机器词典中最长磁条的字符数。  
2.查找机器词典并进行匹配。若匹配成功，则将这个匹配字段作为一个词切分出来。若匹配不成功，则将这个匹配字段的最后一个字去掉，剩下的字符串作为新的匹配字段，进行再次匹配，重复以上过程，直到切分出所有词为止。
- 例一：正向最大匹配法
```
#!/usr/bin/python3
# -*- coding: utf-8 -*-
'''
# @Author: StudentCWZ
# @Date:   2019-11-28 09:20:57
# @Last Modified by:   StudentCWZ
# @Last Modified time: 2019-11-28 09:50:16
'''

class MM(object):
	def __init__(self):
		self.window_size = 3

	def cut(self, text):
		result = []
		index = 0
		text_length = len(text)
		dic = ['研究','研究生','生命','命','的','起源']
		while text_length > index:
			for size in range(self.window_size + index, index, -1): # 3,0,-1
				piece = text[index:size]
				if piece in dic:
					index = size - 1
					break
			index = index + 1
			result.append(piece+'----')
		return result



if __name__ == '__main__':
	text = '研究生命的起源'
	tokenizer = MM()
	print(tokenizer.cut(text))
-- 上述代码输出结果
['研究生----', '命----', '的----', '起源----'] # 这个结果并不是太让人满意
```
### 逆向最大匹配法
逆向最大匹配法(Reverse Maximum Match Method, RMM法)的基本原理与MM法相同，不同的是分词切分的方向与MM法相反。逆向最大匹配法从被处理文档的末端开始匹配扫描，每次取最末端的i个字符(i为字典中最长词数)作为匹配字段，若匹配失败，则去掉匹配字段最前面的一个字，继续匹配。相应地，它使用的分词词典是逆序词典，其中的每个词条都将按逆序方式存放。在实际处理时，先将文档进行到倒排处理，生成逆序文档。然后，根据逆序词典，对逆序文档用正向最大匹配法处理即可。由于汉语中偏正结构较多，若从后向前匹配，可以适当提高精度。所以，逆向最大匹配法比正向最大匹配法的误差要小。统计结果表明，单纯使用正向最大匹配的错误率为1/169，单纯使用逆向最大匹配法的错误率为1/245。
- 例一：逆向最大匹配法
```
#!/usr/bin/python3
# -*- coding: utf-8 -*-
'''
# @Author: StudentCWZ
# @Date:   2019-11-28 10:33:34
# @Last Modified by:   StudentCWZ
# @Last Modified time: 2019-11-28 10:46:39
'''

class RMM(object):
	def __init__(self):
		self.window_size = 3

	def cut(self, text):
		result = []
		index = len(text)
		dic = ['研究','研究生','生命','命','的','起源']
		while index > 0:
			for size in range(index - self.window_size, index):
				piece = text[size:index]
				if piece in dic:
					index = size + 1
					break
			index = index - 1
			result.append(piece+'----')
		result.reverse()
		return result


if __name__ == '__main__':
	text = '研究生命的起源'
	tokenizer = RMM()
	print(tokenizer.cut(text))
-- 上述代码输出结果
['研究----', '生命----', '的----', '起源----']
```
### 双向最大匹配法
双向最大匹配法(Bi-directction Matching Method)是将正向最大匹配法得到的分词结果和逆向最大匹配法得到的结果进行比较，然后按照最大匹配原则，选取词数切分最少的作为结果。双向最大匹配法的规则是：  
1.如果正反向分词结果词数不同，则取分词数量较少的那个。  
2.如果分词结果词数相同：  
(1) 分词结果相同，就说明没有歧义，可以返回任意一个。  
(2) 分词结果不同，返回其中单字较少的那个。
## 统计分词
统计分词其主要思想是把每个词看做是由词的最小单位的各个字组成的，如果相连的字在不同文本中出现的次数越多，就证明这相连的字很可能就是一个词。因此我们就可以利用字与字相邻出现的频率来反应成词的可靠度，统计语料中相邻共现的各个字的组合的频度，当组合频度高于某一个临界值时，我们便可认为此字组可能构成一个词语。基于统计的分词，一般要做如下两步操作：  
1.建立统计语言模型  
2.对句子进行单词划分，然后对划分结果进行概率计算，获得概率最大的分词方式。这里就用到了统计学习算法，如隐含马尔可夫(HMM)、条件随机场(CRF)等。  
## 混合分词
在实际工程应用中，多是基于一种分词算法，然后用其他分词算法加以辅助。最常用的方式就是先基于词典的方式进行分词，然后再用统计分词的方法进行辅助。如此，能在保证词典分词准确率的基础上，对未登录词和歧义词有较好的识别。
### 中文分词工具——jieba
jieba分词结合了基于规则和基于统计这两类方法。
#### jieba安装
- Mac平台下安装
```
1.jieba分词官网：https://pypi.org/project/jieba/
2.在官网中下载jieba压缩包
3.将压缩包解压到anaconda的pkgs目录下:Mac的anaconda（名字可能是anaconda3）在根目录下
4.打开终端，通过指令cd anaconda3/pkgs/jieba-0.39进入pkgs目录下的jieba-0.39（或其他版本），可以看到该文件夹内有setup.py这一安装文件。输入python setup.py install即可完成安装。
```
#### jieba的三种分词模式
jieba提供了三种分词模式：  
1.精确模式：试图将句子最精确地分开，适合文本分析。  
2.全模式：把句子中所有可以成词的词语都扫描出来，速度非常快，但是不能解决歧义。  
3.搜索引擎模式：在精确模式的基础上，对长词再次划分，提高召回率，适合用于搜索引擎分词。  
- 例一：jieba的三种分词模式
```
#!/usr/bin/python3
# -*- coding: utf-8 -*-
'''
# @Author: StudentCWZ
# @Date:   2019-11-28 10:33:34
# @Last Modified by:   StudentCWZ
# @Last Modified time: 2019-11-28 10:46:39
'''

import jieba
sent = '中文分词是文本处理不可或缺的一步！'
seg_list = jieba.cut(sent, cut_all=True)
print('全模式：', '/'.join(seg_list))
seg_list = jieba.cut(sent, cut_all=False)
print('精确模式：', '/'.join(seg_list))
seg_list = jieba.cut(sent)
print('默认精准模式: ', '/'.join(seg_list))
seg_list = jieba.cut_for_search(sent)
print('搜索引擎模式: ', '/'.join(seg_list))
-- 上述代码输出结果
全模式： 中文/分词/是/文本/文本处理/本处/处理/不可/不可或缺/或缺/的/一步//
精确模式： 中文/分词/是/文本处理/不可或缺/的/一步/！
默认精准模式:  中文/分词/是/文本处理/不可或缺/的/一步/！
搜索引擎模式:  中文/分词/是/文本/本处/处理/文本处理/不可/或缺/不可或缺/的/一步/！
```
#### 实战之高频词提取
高频词一般是指文档中出现频率较高且非无用的词语，其一定程度上代表文档的焦点所在。高频词提取其实就是自然语言处理中的TF(Term Frequency)策略。其主要有如下干扰项：  
1.标点符号：一般标点符号无任何价值，需要去除。  
2.停用词：诸如“的”“是”“了”等常用词无任何意义，也需要剔除。  
- 例一：实战之高频词提取
```
# -*- coding: utf-8 -*-
# @Author: StudentCWZ
# @Date:   2019-12-09 15:46:56
# @Last Modified by:   StudentCWZ
# @Last Modified time: 2020-02-03 11:56:27

# 导入第三方库
import glob
import random
import jieba

def get_content(path):
    '''读取语料'''
    with open(path, 'r', encoding='gbk', errors='ignore') as f:
        content = ''
        for l in f:
            l = l.strip()
            content += l
        return content


def get_TF(words, topK=10):
    '''计算高频词'''
    tf_dic = {}
    for w in words:
        tf_dic[w] = tf_dic.get(w, 0) + 1
    return sorted(tf_dic.items(), key = lambda x: x[1], reverse=True)[:topK]




def stop_words(path):
    '''读取停用词'''
    with open(path) as f:
        return [l.strip() for l in f]



def main():
    files = glob.glob('/Users/mac/Desktop/Python自然语言处理实战/data/news/C000013/*.txt')
    corpus = [get_content(x) for x in files]

    sample_inx = random.randint(0, len(corpus)) # 获取随机样本语料

    # split_words = list(jieba.cut(corpus[sample_inx]))
    split_words = [x for x in jieba.cut(corpus[sample_inx]) if x not in stop_words('/Users/mac/Desktop/Python自然语言处理实战/data/stop_words.utf8')]# 过滤停用词，切分语料
    print('样本之一：'+corpus[sample_inx])
    print('样本分词效果：'+ '/ '.join(split_words))
    print('样本的topk(10)词：' + str(get_TF(split_words)))

if __name__ == '__main__':
    main()
-- 上述代码输出结果
样本之一：芦荟，并不稀奇，许多美容品中都含有芦荟的成分。稀奇的是，直接把芦荟鲜叶涂在脸上的做法。这种被称为原汁原味的美容法一时间流行开来，到商场去买芦荟叶也成为新的流行行为。在这里，我们介绍一些芦荟美容的小常识，或许能有助于大家正确使用芦荟美容。芦荟原意是苦味：芦荟，在英语中是苦味之意。属百合科肉质多浆植物，原生长在非洲沿海大陆干旱地带，芦荟种类繁多，不同种的茎叶形态类型也不同，大的像椰子树那样有20多米高，而小的仅有3厘米左右。ban开芦荟厚厚的叶子，可见稠厚透明的肉浆，手感粘滑，气味清淡，微苦。在众多的芦荟品种中，除了作为观赏植物的以外，主要被利用的有：作为药品使用的南部非洲的“开普芦荟”(也叫好望角芦荟)，作为饮料或化妆品使用的美洲的“翠叶芦荟”(也叫库拉索芦荟)和在日本作为食品及化妆品使用的：“木立芦荟”，还有适于外用的“皂质芦荟”和“中华芦荟”。芦荟鲜叶是最适宜直接美容的翠叶芦荟，即库拉索芦荟，它具有使皮肤收敛、柔软化、保湿、消炎、漂白的性能。还有解除硬化、角化、改善伤痕的作用，不仅能小皱纹、眼袋、皮肤松弛，还能保持皮肤湿润、娇嫩，同时，还可以治疗皮肤炎症，对粉刺、雀斑、痤疮以及烫伤、刀伤、虫咬亦有很好的疗效。对头发也同样有效，能使头发保持湿润光滑、预防脱发。怎样用芦荟美容一、面部美容：用鲜叶汁早晚涂于面部15－20分钟，坚持下去，会使面部皮肤光滑、白嫩、柔软，还有治疗蝴蝶斑、雀斑、老年斑的功效。二、自制芦荟化妆水：取汁，加少许水即可涂于面部美容，洗头后抹到头发上可以止痒、防止白发、脱发，并保持头发乌黑发亮，秃顶者还可生出新发。三、自制芦荟润肌膏：备芦荟叶250克、黄瓜1条、鸡蛋1个、面粉和砂糖。1)将芦荟叶片、黄瓜洗净分别弄碎，用纱布取叶；2)将鸡蛋打到碗内，再放入一小匙芦荟汁，3小匙黄瓜汁；2小匙砂糖并充分搅拌混合；3)加入5小匙左右的面粉或燕麦粉，调成膏状即可；4)美容方法：将润肌膏均匀敷在整个脸上，然后，眼、嘴闭合，使面部肌肉保持不动，约40－50分钟后，用温水洗脸。每周坚持1－2次。
样本分词效果：芦荟/ 稀奇/ 美容品/ 中/ 含有/ 芦荟/ 成分/ 稀奇/ 直接/ 芦荟/ 鲜叶/ 涂/ 脸上/ 做法/ 这种/ 称为/ 原汁原味/ 美容/ 法/ 一时间/ 流行/ 开来/ 商场/ 买/ 芦荟/ 叶/ 成为/ 新/ 流行/ 行为/ 介绍/ 芦荟/ 美容/ 小常识/ 或许/ 有助于/ 正确/ 使用/ 芦荟/ 美容/ 芦荟/ 原意/ 苦味/ 芦荟/ 英语/ 中是/ 苦味/ 之意/ 属/ 百合科/ 肉质/ 多浆/ 植物/ 原/ 生长/ 非洲/ 沿海/ 大陆/ 干旱/ 地带/ 芦荟/ 种类/ 繁多/ 不同/ 种/ 茎/ 叶/ 形态/ 类型/ 不同/ 椰子树/ 20/ 多米/ 高/ 仅/ 厘米/ 左右/ ban/ 开/ 芦荟/ 厚厚的/ 叶子/ 稠厚/ 透明/ 肉浆/ 手感/ 粘滑/ 气味/ 清淡/ 微苦/ 众多/ 芦荟/ 品种/ 中/ 观赏植物/ 以外/ 主要/ 利用/ 药品/ 使用/ 南部非洲/ 开普/ 芦荟/ 好望角/ 芦荟/ 饮料/ 化妆品/ 使用/ 美洲/ 翠叶/ 芦荟/ 库拉索/ 芦荟/ 日本/ 食品/ 化妆品/ 使用/ 木立/ 芦荟/ 适于/ 外用/ 皂质/ 芦荟/ 中华/ 芦荟/ 芦荟/ 鲜叶/ 适宜/ 直接/ 美容/ 翠叶/ 芦荟/ 库拉索/ 芦荟/ 具有/ 皮肤/ 收敛/ 柔软/ 化/ 保湿/ 消炎/ 漂白/ 性能/ 解除/ 硬化/ 角化/ 改善/ 伤痕/ 作用/ 皱纹/ 眼袋/ 皮肤/ 松弛/ 保持/ 皮肤/ 湿润/ 娇嫩/ 治疗/ 皮肤/ 炎症/ 粉刺/ 雀斑/ 痤疮/ 烫伤/ 刀伤/ 虫/ 咬/ 疗效/ 头发/ 同样/ 有效/ 头发/ 保持/ 湿润/ 光滑/ 预防/ 脱发/ 芦荟/ 美容/ 面部/ 美容/ 鲜叶/ 汁/ 早晚/ 涂于/ 面部/ 15/ －/ 20/ 分钟/ 坚持下去/ 会/ 面部皮肤/ 光滑/ 白嫩/ 柔软/ 治疗/ 蝴蝶斑/ 雀斑/ 老年斑/ 功效/ 二/ 自制/ 芦荟/ 化妆水/ 取汁/ 加/ 少许/ 水/ 即可/ 涂于/ 面部/ 美容/ 洗头/ 抹/ 头发/ 止痒/ 防止/ 白发/ 脱发/ 保持/ 头发/ 乌黑/ 发亮/ 秃顶/ 生出/ 新/ 发/ 三/ 自制/ 芦荟/ 润/ 肌膏/ 备/ 芦荟/ 叶/ 250/ 克/ 黄瓜/ 条/ 鸡蛋/ 面粉/ 砂糖/ 芦荟/ 叶片/ 黄瓜/ 洗净/ 弄碎/ 纱布/ 取叶/ 鸡蛋/ 打到/ 碗/ 放入/ 小匙/ 芦荟汁/ 小匙/ 黄瓜汁/ 小匙/ 砂糖/ 充分/ 搅拌/ 混合/ 加入/ 小匙/ 左右/ 面粉/ 燕麦/ 粉/ 调/ 成/ 膏状/ 即可/ 美容/ 方法/ 将润/ 肌膏/ 均匀/ 敷/ 整个/ 脸上/ 眼/ 嘴/ 闭合/ 面部/ 肌肉/ 保持/ 动/ 约/ 40/ －/ 50/ 分钟/ 温水/ 洗脸/ 每周/ 坚持/ －/ 次
样本的topk(10)词：[('芦荟', 26), ('美容', 8), ('使用', 4), ('皮肤', 4), ('保持', 4), ('头发', 4), ('面部', 4), ('小匙', 4), ('鲜叶', 3), ('叶', 3)]
```
## 本章小结
本章主要介绍了中文分词的相关技术，并展示基于词典匹配和基于HMM匹配的分词方法。然后详细介绍了jieba分词工具，并结合高频词提取案例，讲解了在实际项目中如何使用。
