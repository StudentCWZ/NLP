# 第四章 词性标注与命名实体识别
## 词性标注
### 词性标注简介
1. 词性是词汇基本的语法属性，通常也称为词类。
2. 词性标注是在给定句子中判定每个词的语法范畴，确定其词性并加以标注的过程。
### 词性标注规范
1. 词性标注需要有一定的标注规范，如将词分为名词、形容词、动词、然后用“n”“adj”“v”等来进行表示。
2. 中文领域中尚无统一的标注标准，较为主流的主要为北大的词性标注集和宾州词性标注集两大类。
### jieba分词中的词性标注
- jieba的词性标注同样是结合规则和统计方式，具体为在词性标记的过程中，词典匹配和HMM共同作用。
- 词性标注流程如下：  
1. 首先基于正则表达式进行汉字判断，正则表达式如下：
```
re_han_internal = re.compile("([\uE00-\u9FD5a-zA-Z0-9+#&\._]+)")
```
2. 若符合上面的正则表达式，则判定为汉字，然后基于前缀词典构建有向无环图，再基于有向无环图计算最大概率路径，同时在前缀词典中找出它所分的词性，若在词典中未找到，则赋予词性为“x”(代表未知)。当然，若在这个过程中，设置使用HMM，且带标注词为未登录词，则会通过HMM方式进行词性标注。  
3. 若不符合上面的正则表达式，那么将继续通过正则表达式进行类型判断，分别赋予“x”“m(数词)”“eng”(英文)。  
- 例一：利用jieba分词进行词性标注的示例如下：
```
#!/usr/bin/python3
# -*- coding: utf-8 -*-
'''
# @Author: StudentCWZ
# @Date:   2019-11-28 10:33:34
# @Last Modified by:   StudentCWZ
# @Last Modified time: 2019-11-28 10:46:39
'''
import jieba.posseg as psg

sent = r'中文分词是文本处理不可或缺的一步！'
seg_list = psg.cut(sent)
print(''.join(['{0}/{1}'.format(w, t) for w,t in seg_list]))
-- 上述代码输出结果
中文/nz分词/n是/v文本处理/n不可或缺/l的/uj一步/m！/x
```
## 命名实体识别
### 命名实体简介
1. 与自动分词、词性标注一样，命名实体识别也是自然语言处理的一个基础任务，是信息抽取、信息检索、机器翻译、问答系统等多种自然语言处理技术必不可少的组成部分。其目的是识别语料中人名、地名、组织结构名等命名实体。
2. 命名实体识别主要有三种方式：
```
1. 基于规则的命名实体识别  
2. 基于统计的命名实体识别  
3. 混合方法(结合规则和统计方法)
```
### 实战：日期识别
- 例一：日期识别
```
# -*- coding: utf-8 -*-
# @Author: StudentCWZ
# @Date:   2019-12-11 17:40:40
# @Last Modified by:   StudentCWZ
# @Last Modified time: 2020-02-05 00:54:55


import re
from datetime import datetime, timedelta
from dateutil.parser import parse
import jieba.posseg as psg


def time_extract(text):
    time_res = []
    word = ''
    keyDate = {'今天':0, '明天':1, '后天':2}
    for k, v in psg.cut(text):
        if k in keyDate:
            if word != '':
                time_res.append(word)
                word = (datetime.today() + timedelta(days=keyDate.get(k, 0))).strftime('%Y年%m月%d日')
        elif word != '':
            if v in ['m', 't']:
                word = word + k
            else:
                time_res.append(word)
                word = ''
        elif v in ['m', 't']:
            word = k
    if word != '':
        time_res.append(word)
    result = list(filter(lambda x: x is not None, [check_time_valid(w) for w in time_res]))
    final_res = [parse_datetime(w) for w in result]
    return [x for x in final_res if x is not None]


def check_time_valid(word):
    m = re.match("\d+$", word)
    if m:
        if len(word) <= 6:
            return None
    word1 = re.sub('[号|日]\d+$', '日', word)
    if word1 != word:
        return check_time_valid(word1)
    else:
        return word1


def parse_datetime(msg):
    if msg is None or len(msg) == 0:
        return None
    try:
        dt = parse(msg, fuzzy=True)
        return dt.strftime('%Y-%m-%d %H:%M:%S')
    except Exception as e:
        m = re.match(r"([0-9零一二两三四五六七八九十]+年)?([0-9一二两三四五六七八九十]+月)?([0-9一二两三四五六七八九十]+[号日])?([上中下午晚早]+)?([0-9零一二两三四五六七八九十百]+[点:\.时])?([0-9零一二三四五六七八九十百]+分?)?([0-9零一二三四五六七八九十百]+秒)?", msg)
        if m.group(0) is not None:
            res = {
            "year":m.group(1),
            "month":m.group(2),
            "day":m.group(3),
            "hour":m.group(5) if m.group(5) is not None else '00',
            "minute":m.group(6) if m.group(6) is not None else '00',
            "second":m.group(7) if m.group(7) is not None else '00',
            }
            params = {}
            for name in res:
                if res[name] is not None and len(res[name]) != 0:
                    tmp = None
                    if name =='year':
                        tmp = year2dig(res[name][:-1])
                    else:
                        tmp = cn2dig(res[name][:-1])
                    if tmp is not None:
                        params[name] = int(tmp)
            target_data = datetime.today().replace(**params)
            is_pm = m.group(4)
            if is_pm is not None:
                if is_pm is not None:
                    if is_pm == u'下午' or is_pm == u'晚上' or is_pm == '中午':
                        hour = target_data.time().hour
                        if hour < 12:
                            target_data = target_data.replace(hour = hour +12)
            return target_data.strftime('%Y-%m-%d %H:%M:%S')
        else:
            return None

UTIL_CN_NUM = {'零':0, '一':1, '二':2, '两':2, '三':3, '四':4, '五':5, '六':6, '七':7, '八':8, '九':9, '0':0, '1':1, '2':2, '3':3, '4':4, '5':5, '6':6, '7':7, '8':8, '9':9 }
UTIL_CN_UNIT = {'十':10, '百':100, '千':1000, '万':10000}

def cn2dig(src):
    if src == "":
        return None
    m = re.match("\d+", src)
    if m:
        return int(m.group(0))
    rsl = 0
    unit = 1
    for item in src[::-1]:
        if item in UTIL_CN_UNIT.keys():
            unit = UTIL_CN_UNIT[item]
        elif item in UTIL_CN_NUM.keys():
            num = UTIL_CN_NUM[item]
            rsl += num * unit
        else:
            return None
    if rsl < unit:
        rsl += unit
    return rsl

def year2dig(year):
    res = ''
    for item in year:
        if item in year:
            if item in UTIL_CN_NUM.keys():
                res = res + str(UTIL_CN_NUM(item))
            else:
                res = res + item
    m = re.match("\d+", res)
    if m:
        if len(m.group(0)) == 2:
            return int(datetime.datetime.today().year/100)*100 + int(m.group(0))
        else:
            return int(m.group(0))
    else:
        return None

text1 = '我要住到明天下午三点'
print(text1, time_extract(text1), sep=':')
text2 = '预定28号的房间'
print(text2, time_extract(text2), sep=':')
text3 = '我要从26号下午4点住到11月2号'
print(text3, time_extract(text3), sep=':')

-- 上述代码输出结果
我要住到明天下午三点:['2020-02-05 15:00:00']
预定28号的房间:['2020-02-28 00:00:00']
我要从26号下午4点住到11月2号:['2020-02-26 16:00:00', '2020-11-02 00:00:00']
```
### 实战：地名识别
1. 在日期识别中，我们主要采用了基于规则(正则表达式)的方式。
2. 在本节，我们将采用基于条件随机场的方法来完成地名识别任务。  
#### Mac平台下安装CRF++  
1. 利用git安装(安装网址：https://github.com/taku910/crfpp.git)  
2. 将文件拉取到本地  
```
git clone https://github.com/taku910/crfpp.git
```
3. 进入仓库  
```
cd crfpp
```
4. 新建winmain.h文件，并修改winmain.h文件
```
touch 'winmain.h'
vi 'winmain.h'
```
5. 修改的winmain.h的文件内容
```
//
//  CRF++ -- Yet Another CRF toolkit
//
//  $Id: common.h 1588 2007-02-12 09:03:39Z taku $;
//
//  Copyright(C) 2005-2007 Taku Kudo <taku@chasen.org>
//
#if defined(_WIN32) || defined(__CYGWIN__)

#include <windows.h>
#include <string>

namespace {
class CommandLine {
 public:
  CommandLine(int argc, wchar_t **argv) : argc_(argc), argv_(0) {
    argv_ = new char * [argc_];
    for (int i = 0; i < argc_; ++i) {
      const std::string arg = WideToUtf8(argv[i]);
      argv_[i] = new char[arg.size() + 1];
      ::memcpy(argv_[i], arg.data(), arg.size());
      argv_[i][arg.size()] = '\0';
    }
  }
  ~CommandLine() {
    for (int i = 0; i < argc_; ++i) {
      delete [] argv_[i];
    }
    delete [] argv_;
  }

  int argc() const { return argc_; }
  char **argv() const { return argv_; }

 private:
  static std::string WideToUtf8(const std::wstring &input) {
    const int output_length = ::WideCharToMultiByte(CP_UTF8, 0,
                                                    input.c_str(), -1, NULL, 0,
                                                    NULL, NULL);
    if (output_length == 0) {
      return "";
    }

    char *input_encoded = new char[output_length + 1];
    const int result = ::WideCharToMultiByte(CP_UTF8, 0, input.c_str(), -1,
                                             input_encoded,
                                             output_length + 1, NULL, NULL);
    std::string output;
    if (result > 0) {
      output.assign(input_encoded);
    }
    delete [] input_encoded;
    return output;
  }

  int argc_;
  char **argv_;
};
}  // namespace

#define main(argc, argv) wmain_to_main_wrapper(argc, argv)

int wmain_to_main_wrapper(int argc, char **argv);

int wmain(int argc, wchar_t **argv) {
  CommandLine cmd(argc, argv);
  return wmain_to_main_wrapper(cmd.argc(), cmd.argv());
}
#endif
```
6. 进行下面操作
```
./configure
make
sudo make install
```
7. 进入仓库中python目录，进行如下操作
```
cd python
python setup.py build
sudo python setup.py install
```
8. 安装完成，验证安装是否成功(进入终端输入以下指令)
```
crf_learn
```
#### 语料数据处理  
1. CRF++的训练数据要求一定格式，一般是一行一个token，一句话有多行token组成，多个句子之间用空行分开。其中每行又分成多列，除最后一列外，其他列表示特征。
2. 因此，一般至少需要两列，最后一列表示要预测的标签(“B”、“E”、“M”、“S”、“O”)
- 例一：数据处理代码
```
#!/usr/bin/python3
# -*- coding: utf-8 -*-
'''
# @Author: StudentCWZ
# @Date:   2020-02-05 12:22:58
# @Last Modified by:   StudentCWZ
# @Last Modified time: 2020-02-05 17:15:54
'''



import CRFPP

def tag_line(words, mark):
    chars = []
    tags = []
    temp_word = '' #用于合并组合词
    for word in words:
        word = word.strip('\t ')
        if temp_word == '':
            bracket_pos = word.find('[')
            w, h = word.split('/')
            if bracket_pos == -1:
                if len(w) == 0: continue
                chars.extend(w)
                if h == 'ns':
                    tags += ['S'] if len(w) == 1 else ['B'] + ['M'] * (len(w) - 2) + ['E']
                else:
                    tags += ['O'] * len(w)
            else:
                w = w[bracket_pos+1:]
                temp_word += w
        else:
            bracket_pos = word.find(']')
            w, h = word.split('/')
            if bracket_pos == -1:
                temp_word += w
            else:
                w = temp_word + w
                h = word[bracket_pos+1:]
                temp_word = ''
                if len(w) == 0: continue
                chars.extend(w)
                if h == 'ns':
                    tags += ['S'] if len(w) == 1 else ['B'] + ['M'] * (len(w) - 2) + ['E']
                else:
                    tags += ['O'] * len(w)

    assert temp_word == ''
    return (chars, tags)

def corpusHandler(corpusPath):
    import os
    root = os.path.dirname(corpusPath)
    with open(corpusPath) as corpus_f, \
        open(os.path.join(root, 'train.txt'), 'w') as train_f, \
        open(os.path.join(root, 'test.txt'), 'w') as test_f:

        pos = 0
        for line in  corpus_f:
            line = line.strip('\r\n\t ')
            if line == '': continue
            isTest = True if pos % 5 == 0 else False  # 抽样20%作为测试集使用
            words = line.split()[1:]
            if len(words) == 0: continue
            line_chars, line_tags = tag_line(words, pos)
            saveObj = test_f if isTest else train_f
            for k, v in enumerate(line_chars):
                saveObj.write(v + '\t' + line_tags[k] + '\n')
            saveObj.write('\n')
            pos += 1

if __name__ == '__main__':
    corpusHandler('/Users/mac/Desktop/Python自然语言处理实战/people-daily.txt')
-- 上述代码输出结果
# 输出两个文件：test.txt、train.txt
```
#### 特征模板设计  
1. 在介绍基于条件随机场的命名实体识别时，提到了CRF有特征函数，它是通过定义一些规则来实现的，而这些规则就对应着CRF++中的特征模板。其基本格式为%x[row,col]，用于确定输入数据的一个token，其中，row确定与当前的token的相对行数，col用于确定决定列数。
- 例一：特征模板
temolate文件定义特征模板
```
#Unigram
U00:%x[-1,0]
U01:%x[0,0]
U02:%x[1,0]
U03:%x[2,0]
U04:%x[-2,0]
U05:%x[1,0]/%x[2,0]
U06:%x[0,0]/%x[-1,0]/%x[-2,0]
U07:%x[0,0]/%x[1,0]/%x[2,0]
U08:%x[-1,0]/%x[0,0]
U09:%x[0,0]/%x[1,0]
U10:%x[-1,0]/%x[1,0]

#Bigram
B
```
#### 模型训练和测试  
1. CRF++提供了训练和测试的命令：crf_learn、crf_test。
- 例一：模型训练
训练时采用如下的命令：
```
crf_learn -f 4 -p 8 -c 3 /Users/mac/Desktop/Python自然语言处理实战/template /Users/mac/Desktop/Python自然语言处理实战/train.txt /Users/mac/Desktop/Python自然语言处理实战/model
-- crf_learn的参数含义
 -f, --freq=INT              use features that occuer no less than INT(default 1)
 -m, --maxiter=INT           set INT for max iterations in LBFGS routine(default 10k)
 -c, --cost=FLOAT            set FLOAT for cost parameter(default 1.0)
 -e, --eta=FLOAT             set FLOAT for termination criterion(default 0.0001)
 -C, --convert               convert text model to binary model
 -t, --textmodel             build also text model file for debugging
 -a, --algorithm=(CRF|MIRA)  select training algorithm
 -p, --thread=INT            number of threads (default auto-detect)
 -H, --shrinking-size=INT    set INT for number of iterations variable needs to  be optimal before considered for shrinking. (default 20)
 -v, --version               show the version and exit
 -h, --help                  show this help and exit
-- 上述代码部分输出结果
Number of sentences: 15586
Number of features:  311708
Number of thread(s): 8
Freq:                4
eta:                 0.00010
C:                   3.00000
shrinking size:      20
iter=0 terr=0.99596 serr=1.00000 act=311708 obj=6165011.83405 diff=1.00000
iter=1 terr=0.03156 serr=0.44360 act=311708 obj=2262704.65902 diff=0.63298
iter=2 terr=0.03156 serr=0.44360 act=311708 obj=600917.06813 diff=0.73443
iter=3 terr=0.03156 serr=0.44360 act=311708 obj=574673.69795 diff=0.04367
iter=4 terr=0.03156 serr=0.44360 act=311708 obj=476257.63429 diff=0.17126
iter=5 terr=0.97653 serr=1.00000 act=311708 obj=21533880.38229 diff=44.21477
iter=6 terr=0.03156 serr=0.44360 act=311708 obj=463091.80551 diff=0.97849
iter=7 terr=0.03156 serr=0.44367 act=311708 obj=501052.82980 diff=0.08197
iter=8 terr=0.03156 serr=0.44360 act=311708 obj=358010.82199 diff=0.28548
iter=9 terr=0.96945 serr=0.98884 act=311708 obj=1996891.04560 diff=4.57774
iter=10 terr=0.03156 serr=0.44360 act=311708 obj=328267.25145 diff=0.83561
iter=11 terr=0.97466 serr=0.99365 act=311708 obj=3111421.76063 diff=8.47832
```
- 例二：模型测试
1. 测试时采用如下命令：
```
crf_test -m /Users/mac/Desktop/Python自然语言处理实战/model /Users/mac/Desktop/Python自然语言处理实战/test.txt > /Users/mac/Desktop/Python自然语言处理实战/test.rst
```
- 例三：验证模型在测试集上的效果
```
#!/usr/bin/python3
# -*- coding: utf-8 -*-
'''
# @Author: StudentCWZ
# @Date:   2020-02-05 23:05:53
# @Last Modified by:   StudentCWZ
# @Last Modified time: 2020-02-05 23:28:38
'''

def f1(path):

    with open(path) as f:

        all_tag = 0 #记录所有的标记数
        loc_tag = 0 #记录真实的地理位置标记数
        pred_loc_tag = 0 #记录预测的地理位置标记数
        correct_tag = 0 #记录正确的标记数
        correct_loc_tag = 0 #记录正确的地理位置标记数

        states = ['B', 'M', 'E', 'S']
        for line in f:
            line = line.strip()
            if line == '': continue
            _, r, p = line.split()

            all_tag += 1

            if r == p:
                correct_tag += 1
                if r in states:
                    correct_loc_tag += 1
            if r in states: loc_tag += 1
            if p in states: pred_loc_tag += 1

        loc_P = 1.0 * correct_loc_tag/pred_loc_tag
        loc_R = 1.0 * correct_loc_tag/loc_tag
        print('loc_P:{0}, loc_R:{1}, loc_F1:{2}'.format(loc_P, loc_R, (2*loc_P*loc_R)/(loc_P+loc_R)))


if __name__ == '__main__':
	f1('/Users/mac/Desktop/Python自然语言处理实战/test.rst')
-- 上述代码输出结果
loc_P:0.842753099795, loc_R:0.801230329041, loc_F1:0.821467335504
```
#### 模型使用
- 例一：模型使用
```
#!/usr/bin/python3
# -*- coding: utf-8 -*-
'''
# @Author: StudentCWZ
# @Date:   2020-02-05 23:05:53
# @Last Modified by:   StudentCWZ
# @Last Modified time: 2020-02-06 01:28:58
'''

def load_model(path):
    import os, CRFPP
    # -v 3: access deep information like alpha,beta,prob
    # -nN: enable nbest output. N should be >= 2
    if os.path.exists(path):
        return CRFPP.Tagger('-m {0} -v 3 -n2'.format(path))
    return None

def locationNER(text):

    tagger = load_model('/Users/mac/Desktop/Python自然语言处理实战/model')

    for c in text:
        tagger.add(c)

    result = []

    # parse and change internal stated as 'parsed'
    tagger.parse()
    word = ''
    for i in range(0, tagger.size()):
        for j in range(0, tagger.xsize()):
            ch = tagger.x(i, j)
            tag = tagger.y2(i)
            if tag == 'B':
                word = ch
            elif tag == 'M':
                word += ch
            elif tag == 'E':
                word += ch
                result.append(word)
            elif tag == 'S':
                word = ch
                result.append(word)


    return result


if __name__ == '__main__':
    # f1('/Users/mac/Desktop/Python自然语言处理实战/test.rst')
    text = '我中午要去北京饭店，下午去中山公园，晚上回亚运村。'
    print(text, locationNER(text), sep='==> ')

    text = '我去回龙观，不去南锣鼓巷'
    print(text, locationNER(text), sep='==> ')

    text = '打的去北京南站地铁站'
    print(text, locationNER(text), sep='==> ')
-- 上述代码输出结果
我中午要去北京饭店，下午去中山公园，晚上回亚运村。==> ['北京饭店', '中山公园', '亚运村']
我去回龙观，不去南锣鼓巷==> []
打的去北京南站地铁站==> ['北京']
```
1. 注意：可以看到，该程序针对一些场景能够很好地进行识别，但是在遇到诸如“回龙观”“南锣鼓巷”“北京南站”等词时识别效果不好。
2. 这种情况在实际项目中会经常遇到，通常的解决办法是： 
```
1. 扩展语料，改进模型。如加入词性特征，调整分词算法等。  
2. 整理地理位置词库。在识别时，先通过词库匹配，再采用模型发现。
```


