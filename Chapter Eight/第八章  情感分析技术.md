# 第八章 情感分析技术
## 情感分析的应用
1.在日常生活中，情感分析的应用非常普遍，下面列举几个常见的应用渠道：  
(1) 电子商务：情感分析最长应用到的领域就是电子商务。  
(2) 舆情分析：无论政府还是公司，都需要不断监控社会对于自身的舆论态度。  
(3) 市场呼声：市场呼声是指消费者使用竞争对手提供的产品与服务的感受。  
(4) 消费者呼声：消费者呼声是指个体消费者对产品与服务的评价。
## 情感分析的基本方法
1.根据分析载体的不同，情感分析就会涉及很多主题，包括针对电影评论、商品评论，以及新闻和博客等的情感分析。对情感分析的研究到目前为止主要集中在两个方面：识别给定的文本实体是主观还是客观的，以及识别主观的文本的极性。大多数情感分析研究都使用机器学习方法。  
2.在情感分析领域，文本可以划分为积极和消极两类，或者积极、消极和中性(或不相关)的多类。分析方法主要分为：  
(1) 词法分析  
(2) 基于机器学习的分析  
(3) 混合分析  
### 词法分析
1.词法分析运用了由预标记词汇组成的字典，使用词法分析器将输入文本转换为单词序列。将每个新的单词与字典中的词汇进行匹配。如果有一个积极的匹配，分数加到输入文本的分数总池中。  
2.文本的分类取决于文本的总得分。目前有大量的工作效力于度量词法信息的有效性。对单个短语，通过手动标记词汇(仅包含形容词)的方法，大概能达到85%的准确率，这是由于评价文本的主观性所决定的。有研究者将同样的方法用于电影评论，准确率仅为62%。也有研究者通过简单地从消极词汇集合中去除积极词汇来评价语义差距，得到了82%的准确度。词法分析也存在一个不足：其性能(时间复杂度和准确率)会随着字典大小(词汇的数量)的增加而迅速下降。  
### 机器学习方法
1.机器学习技术由于其具有高的适应性和准确性受到了越来越多的关注。在情感分析中，主要使用的是监督学习方法。它可以分为三个阶段：数据收集、预处理、训练分类。在训练过程中，需要提供一个标记预料库作为训练数据。分类器使用一系列特征向量对目标数据进行分类。在机器学习技术中，决定分类器准确率的关键是合适的特征选择。  
### 混合分析
1.情感分析研究的进步吸引大量研究者开始探讨将两种方法进行组合的可能性，既可以利用机器学习方法的高准确性，又可以利用词法分析快速的特点。有研究者利用由两个词组成的词汇和一个未标记的数据，将这些由两个词组成的词汇划分为积极的类和消极的类。利用被选择的词汇集合中的所有单词产生一些伪文件。然后计算伪文件与未标记文件之间的余弦相似度。根据相似度将这些文件划分为积极的或消极的情感。之后这些训练数据集被送入朴素贝叶斯分类器进行训练。  
## 实战电影评论情感分析
1.在NLP当中，情感分析是一段文字表达的情绪状态。其中，一段文本可以是一个句子、一个段落或者一个文档。情绪状态可以是两类，例如正面、负面，喜悦、忧伤；也可以是这三类，例如积极、中性、消极等。  
2.在NLP问题中，情感分析可以被归类为文本分类问题。主要涉及两个问题：文本表达和文本分类。在深度学习出现之前，主流的表示方法有BOW(词袋模型)和topic model(主题模型)，分类模型主要有SVM(支持向量机)和LR(逻辑回归)。  
3.在文本分类模型方面，一般我们会使用传统机器学习方法，例如支持向量机(Support Vector Machines, SVM)、朴素贝叶斯(Naive bayes, NB)等，或者深度学习相关方法，比如卷积神经网络(Convolutional Neural Networks, CNN)、循环神经网络(Recurrent Neural Networks, RNN)及其变体。
### 卷积神经网络
1.CNN卷积神经网络，一般首先使用卷积操作处理词向量序列序列，生成多通道特征图，对特征图采用时间维度上的最大池化操作得到与此卷积核对应的整句话的特征，最后将所有卷积核得到的特征拼接起来即为文本的定长向量。对于文本分类问题，将其连接至Softmax层即构建完全的模型。
### 循环神经网络
1.循环神经网络是一种能够对时序数据进行精准建模的网络。文本的独特在于是典型的的序列数据，每个文字的出现都是依赖于前面的单词和后面的单词。  
2.RNN及其变种长短时记忆网络(Long Shot Memory, LSTM)在NLP领域得到了广泛应用，例如在语言模型、句法分析、语意角色标注、图说模型、对话、机器翻译等领域均有优异的表现。
### 长短时记忆网络
1.长短时记忆网络单元是RNN的升级版本，由Hochreiter和Schmidhuber(1997)提出，近期被ALex Graves改良。从抽象的角度来看，LSTM保存了文本中的长期依赖信息。LSTM通过对循环层的刻意设计来避免长期依赖问题和梯度消失的问题。  
2.情感分析的任务试分析一句话是积极、消极还是中性的。我们把任务分为五个部分：  
(1) 训练或者载入一个词向量生成模型  
(2) 创建一个用于训练集的ID矩阵  
(3) 创建一个用于LSTM计算单元  
(4) 训练  
(5) 测试
#### 载入数据和数据预处理
1.本节我们以IMDB情感分析数据集为例进行介绍。IMDB数据集的训练集和测试集分别包含了25000条已标注的电影评价。  
2.首先使用已经训练好的词向量模型。
```
#!/usr/bin/python3
# -*- coding: utf-8 -*-
'''
# @Author: StudentCWZ
# @Date:   2019-12-06 19:48:21
# @Last Modified by:   StudentCWZ
# @Last Modified time: 2019-12-06 20:04:16
'''

import numpy as np

wordsList = np.load('wordsList.npy')
print('载入word列表')
wordsList = wordsList.tolist()
wordsList = [word.decode('UTF-8')
             for word in wordsList]
wordVectors = np.load('wordVectors.npy')
print('载入文本向量')

print(len(wordsList))
print(wordVectors.shape)
-- 上述代码输出结果
载入word列表
载入文本向量
400000
(400000, 50)
```
3.数据预处理
```
#!/usr/bin/python3
# -*- coding: utf-8 -*-
'''
# @Author: StudentCWZ
# @Date:   2019-12-06 19:48:21
# @Last Modified by:   StudentCWZ
# @Last Modified time: 2019-12-06 20:04:16
'''

import os
from os.path import isfile, join

pos_files = ['pos/' + f for f in os.listdir(
    'pos/') if isfile(join('pos/', f))]
neg_files = ['neg/' + f for f in os.listdir(
    'neg/') if isfile(join('neg/', f))]
num_words = []
for pf in pos_files:
    with open(pf, "r", encoding='utf-8') as f:
        line = f.readline()
        counter = len(line.split())
        num_words.append(counter)
print('正面评价完结')

for nf in neg_files:
    with open(nf, "r", encoding='utf-8') as f:
        line = f.readline()
        counter = len(line.split())
        num_words.append(counter)
print('负面评价完结')

num_files = len(num_words)
print('文件总数', num_files)
print('所有的词的数量', sum(num_words))
print('平均文件词的长度', sum(num_words) / len(num_words))
-- 上述代码输出结果
正面评价完结
负面评价完结
文件总数 25000
所有的词的数量 5844680
平均文件词的长度 233.7872
```
4.数据可视化
```
#!/usr/bin/python3
# -*- coding: utf-8 -*-
'''
# @Author: StudentCWZ
# @Date:   2019-12-06 19:48:21
# @Last Modified by:   StudentCWZ
# @Last Modified time: 2019-12-06 20:04:16
'''

import re

strip_special_chars = re.compile("[^A-Za-z0-9 ]+")
num_dimensions = 300  # Dimensions for each word vector


def cleanSentences(string):
    string = string.lower().replace("<br />", " ")
    return re.sub(strip_special_chars, "", string.lower())


max_seq_num = 250

ids = np.zeros((num_files, max_seq_num), dtype='int32')
file_count = 0
for pf in pos_files:
  with open(pf, "r", encoding='utf-8') as f:
    indexCounter = 0
    line = f.readline()
    cleanedLine = cleanSentences(line)
    split = cleanedLine.split()
    for word in split:
      try:
        ids[file_count][indexCounter] = wordsList.index(word)
      except ValueError:
        ids[file_count][indexCounter] = 399999  # 未知的词
      indexCounter = indexCounter + 1
      if indexCounter >= max_seq_num:
        break
    file_count = file_count + 1

for nf in neg_files:
  with open(nf, "r",encoding='utf-8') as f:
    indexCounter = 0
    line = f.readline()
    cleanedLine = cleanSentences(line)
    split = cleanedLine.split()
    for word in split:
      try:
        ids[file_count][indexCounter] = wordsList.index(word)
      except ValueError:
        ids[file_count][indexCounter] = 399999  # 未知的词语
      indexCounter = indexCounter + 1
      if indexCounter >= max_seq_num:
        break
    file_count = file_count + 1

np.save('idsMatrix', ids)
```
5.辅助函数：该辅助函数返回一个数据集的迭代器，用于返回一批训练(训练的集合)。
```
#!/usr/bin/python3
# -*- coding: utf-8 -*-
'''
# @Author: StudentCWZ
# @Date:   2019-12-06 19:48:21
# @Last Modified by:   StudentCWZ
# @Last Modified time: 2019-12-06 20:04:16
'''

from random import randint

batch_size = 24
lstm_units = 64
num_labels = 2
iterations = 100
lr = 0.001
ids = np.load('idsMatrix.npy')


def get_train_batch():
    labels = []
    arr = np.zeros([batch_size, max_seq_num])
    for i in range(batch_size):
        if (i % 2 == 0):
            num = randint(1, 11499)
            labels.append([1, 0])
        else:
            num = randint(13499, 24999)
            labels.append([0, 1])
        arr[i] = ids[num - 1:num]
    return arr, labels


def get_test_batch():
    labels = []
    arr = np.zeros([batch_size, max_seq_num])
    for i in range(batch_size):
        num = randint(11499, 13499)
        if (num <= 12499):
            labels.append([1, 0])
        else:
            labels.append([0, 1])
        arr[i] = ids[num - 1:num]
    return arr, labels
```
#### 模型设置
```
#!/usr/bin/python3
# -*- coding: utf-8 -*-
'''
# @Author: StudentCWZ
# @Date:   2019-12-06 19:48:21
# @Last Modified by:   StudentCWZ
# @Last Modified time: 2019-12-06 20:04:16
'''

import tensorflow as tf

tf.reset_default_graph()

labels = tf.placeholder(tf.float32, [batch_size, num_labels])
input_data = tf.placeholder(tf.int32, [batch_size, max_seq_num])
data = tf.Variable(
    tf.zeros([batch_size, max_seq_num, num_dimensions]), dtype=tf.float32)
data = tf.nn.embedding_lookup(wordVectors, input_data)

lstmCell = tf.contrib.rnn.BasicLSTMCell(lstm_units)
lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.5)
value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)

weight = tf.Variable(tf.truncated_normal([lstm_units, num_labels]))
bias = tf.Variable(tf.constant(0.1, shape=[num_labels]))
value = tf.transpose(value, [1, 0, 2])
last = tf.gather(value, int(value.get_shape()[0]) - 1)
prediction = (tf.matmul(last, weight) + bias)

correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
    logits=prediction, labels=labels))
optimizer = tf.train.AdamOptimizer(lr).minimize(loss)

saver = tf.train.Saver()

with tf.Session() as sess:
    if os.path.exists("models") and os.path.exists("models/checkpoint"):
        saver.restore(sess, tf.train.latest_checkpoint('models'))
    else:
        if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:
            init = tf.initialize_all_variables()
        else:
            init = tf.global_variables_initializer()
        sess.run(init)

    iterations = 100
    for step in range(iterations):
        next_batch, next_batch_labels = get_test_batch()
        if step % 20 == 0:
            print("step:", step, " 正确率:", (sess.run(
                accuracy, {input_data: next_batch, labels: next_batch_labels})) * 100)

    if not os.path.exists("models"):
        os.mkdir("models")
    save_path = saver.save(sess, "models/model.ckpt")
    print("Model saved in path: %s" % save_path)
```
#### 调参设置
1.选择合适的参数训练网络非常重要，最终模型的好坏很大程度上取决于你选择的优化器(Momentum、Nesterov、AdaGrad、RMSProp、Adam)、学习率以及网络架构。特别是RNN和LSTM，单元数量和词向量大小都是重要的因素。  
2.Learning Rate: RNN网络最困难的部分就是它的训练速度慢，耗时非常久。所以学习率至关重要。如果学习率设置过大，则学习曲线会有很大的波动性，如果设置过小，则收敛得非常慢。根据经验设置为0.001比较好。如果训练得非常慢，可以适当增加这个值。  
3.优化器: 之所以优化器选择Adam，是因为其广泛被使用。  
4.LSTM细胞数量：这个值取决于输出文本的平均长度。单元数量过多会导致速度非常慢。  
5.词向量维度：词向量一般设置在50~300之间，维度越多可以存放越多信息，但是也意味更高的计算成本。
#### 训练过程
1.使用Tensorflow的基本过程是：先定义一个Tensorflow的会话，如果有GPU选择用GPU运算，然后加载一批文字和对应的标签，之后调用会话的run函数。这个函数有两个参数，第一个称为fetches参数，这个参数定义了我们感兴趣的值。希望通过我们的优化器来最小化损失函数。第二个参数是feed_dict参数，这个参数用来传入占位单元。需要将一批处理的评论和标签输入模型，然后不断对这组数据进行训练。  
2.在tensorboard查看的方法： 
```
tensorboard --logdir=tensorboard
```
3.之后打开浏览器，输入如下代码可以查看训练动态
```
http://localhost:6006/
```
## 本章小结
1.本章我们详细地梳理了情感分析的需求，例如电子商务、舆情分析、市场呼声等。之后介绍了舆情分析的基本原理、方法，以及最新的一些应用场景。在此基础上，在实战内容中，我们还阐述了RNN、LSTM的原理和使用方法，并且引入了一部分Tensorflow的使用方法。
