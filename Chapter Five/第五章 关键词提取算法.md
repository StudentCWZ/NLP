# 第五章 关键词提取算法
## 关键词提取技术概述
关键词提取算法一般也分为有监督和无监督两类。有监督的关键词提取方法主要是通过分类的方式进行，通过构建一个较为丰富和完善的词表，然后通过判断每个文档与词表中每个词的匹配程度，以类似打标签的方式，达到关键词提取的效果。有监督的方法能够获取到较高的精度，但缺点是需要大批量的标注数据，人工成本过高。相对于有监督的方法而言，无监督的方法对数据的要求就低很多了。既不需要一张人工生成、维护的词表，也不需要人工标准预料辅助进行训练。在本章中，主要为大家介绍的就是一些目前较常用的无监督关键词提取算法，分别是TF-IDF算法、TextRank算法和主题模型算法(包括LSA、LSI、LDA等)。
## 关键词提取算法TF-IDF算法
TF-IDF算法(Term Frequency-Inverse Document Frequency,词频-逆文档频次法)是一种基于统计的计算方法，常用于评估在一个文档集中一个词对某份文件的重要程度。这种作用显然很符合关键词抽取的需求，一个词对文档越重要，那就越可能是文档的关键词，人们常将TF-IDF算法应用于关键词提取中。从算法的名称就可以看出，TF-IDF算法由两部分组成：TF算法以及IDF算法。TF算法是统计一个词在一篇文档中出现的频次，其基本思想是，一个词在文档中出现次数越多，则对文档的表达能力也就是越强。而IDF算法则是统计一个词在文档集的多少个文档出现，其基本的思想，如果一个词在越少的文档中出现，则其对文档的区分能力也就越强。TF—IDF算法就是TF算法与IDF算法的综合使用，一般是相乘。
## TextRank算法
1.TextRank算法与本章提到的其他算法都不同的一点是，其他算法的关键词提取都要基于一个现成的语料库。如在TF—IDF中需要统计每个词在预料库中的多少个文档有出现过，也就是逆文档频率；主题模型的关键词提取算法则是要通过对大规模文档的学习，来发现文档的隐含主题。而TextRank算法则是脱离语料库的背景，仅对单篇文档进行分析就可以提取该文档的关键词。这也是TextRank算法的一个重要特点。TextRank算法最早用于文档的自动摘要，基于句子维度的分析，利用TextRank对每个句子进行打分，挑选出分数最高的n个句子作为文档的关键句，以达到自动摘要的效果。  
2.TextRank算法的基本思想来源于Google的PageRank算法。PageRank算法是一种网页排名算法，其基本思想有两条：  
(1) 链接数量。一个网页被越多的其他网页链接，说明这个网页越重要。  
(2) 链接质量。一个网页被一个越高权指的网页链接，也能表明这个网页越重要。  
3.以上就是PageRank的理论，也是TextRank的理论基础。不同的一点是，PageRank是有向无权图，而TextRank进行自动摘要则是有权图，因为在计分时除了考虑链接句的重要性外，还要考虑两个句子间的相似性。当TextRank应用到关键词提取时，与应用在自动摘要中时主要有两点不同：  
(1) 词与词之间的关联没有权重。  
(2) 每个词不是与文档中所有的词都有链接的。
## LSA、LSI、LDA算法
TF-IDF算法和TextRank算法这两种模型是直接根据词与文档的关系，对关键词进行提取。这两种方法仅用到了文本中的统计信息，对文本中丰富的信息无法充分地进行利用，尤其是其中的语义信息，对文本关键词的抽取显然是一种非常有用的信息。与前面两种模型不同的是，主题模型认为在词与文档之间没有直接联系，它们应当还有一个维度将它们串联起来，主题模型将这个维度称为主题。每个文档都应该都对应着一个或多个的主题，而每个主题都会有对应的词分布，通过主题，就可以得到每个文档的词分布。
### LSA、LSI算法
1.LSA(latent Semantic Analysis, 潜在语义分析)和LSI(Latent Semantic Index, 潜在语义索引)，两者通常被认为是同一种算法，只是应用的场景略有不同，LSA是在需要构建的相关任务中的叫法。可以说，LSA和LSI都是对文档的潜在语义进行分析，但是潜在语义索引在分析后，还会利用分析的结果建立相关的索引。  
2.LSA的主要步骤如下：  
(1) 使用BOW模型将每个文档表示为向量  
(2) 将所有的文档词向量拼接起来构成词-文档矩阵(mxn)  
(3) 对词-文档矩阵进行奇异值分解(SVD)操作([mxr][rxr][rxn])  
(4) 根据SVD结果，将词-文档矩阵映射到一个更低维度k([mxk][kxk][kxn], 0<k<r)的近似SVD结果，每个词和文档都可以表示为k个主题构成的空间中的一个点，通过计算每个词和文档的的相似度(相似度计算可以通过余弦相似度或者是KL相似度进行)，可以得到每个文档中对每个词的相似度结果，去相似度最高的一个词即为文档的关键词。  
3.相较于传统SVM模型(Space Vector Model, 空间向量模型)对语义信息利用的缺乏，LSA通过SVD(奇异值分解)将词、文档映射到一个低维的语义空间，挖掘出词、文档的浅层语义信息，从而对词、文档进行更本质地表达。  
4.LSA是通过SVD这一暴力的方法，简单直接地求解出近似的word-topic-document分布信息。但是其作为一个初级的主题模型，仍然存在着许多的不足。其中主要的缺点是：SVD的计算复杂度非常高，特征空间维度较大的，计算效率十分低下。
### LDA算法
1.LDA算法的理论基础是贝叶斯理论。LDA根据词的共现信息的分析，拟合出词-文档-主题的分布，进而将词、文本都映射到一个语义空间中。  
2.结合吉布斯采样的LDA模型训练过程一般如下：  
(1) 随机初始化，对语料中的每篇文档中的每个词w，随机地赋予一个topic编号  
(2) 重新扫描语料库，对每个词w按照吉布斯采样公式重新采样它的topic，在语料中进行更新。  
(3) 重复以上语料库的重新采样过程直到吉布斯采样收敛。  
(4) 统计语料库的topic-word共现频率矩阵，该矩阵就是LDA的模型。  
3.经过以上的步骤，就得到一个训练好的LDA模型，接下来就可以按照一定的方式针对新文档的topic进行预估，具体步骤如下：  
(1) 随机初始化，对当前文档中的每个词w，随机地赋一个topic编号。  
(2) 重新扫描当前文档，按照吉布斯采样公式，重新采样它的topic。  
(3) 重复以上过程直到吉布斯采样收敛  
(4) 统计文档中的topic分布即为预估结果。  
## 实战：提取文本关键词
1.训练一个关键词提取算法需要以下几个步骤：  
(1) 加载已有的文档数据集  
(2) 加载停用词  
(3) 对数据集中的文档进行分词  
(4) 根据停用词表，过滤干扰词  
(5) 根据数据集训练算法  
2.根据训练好的关键词提取算法对新文档进行关键词提取需要经过以下环节：  
(1) 对新文档进行分词  
(2) 加载停用词表，过滤干扰词  
(3) 根据训练好的算法提取关键词
- 例一：提取文本关键词
```
#!/usr/bin/python3
# -*- coding: utf-8 -*-
'''
# @Author: StudentCWZ
# @Date:   2020-02-07 23:02:33
# @Last Modified by:   StudentCWZ
# @Last Modified time: 2020-02-07 23:12:16
'''

import math

import jieba
import jieba.posseg as psg
from gensim import corpora, models
from jieba import analyse
import functools


# 停用词表加载方法
def get_stopword_list():
    # 停用词表存储路径，每一行为一个词，按行读取进行加载
    # 进行编码转换确保匹配准确率
    stop_word_path = '/Users/mac/Desktop/Python自然语言处理实战/stopword.txt'
    stopword_list = [sw.replace('\n', '') for sw in open(stop_word_path,encoding='utf-8').readlines()]
    return stopword_list


# 分词方法，调用结巴接口
def seg_to_list(sentence, pos=False):
    if not pos:
        # 不进行词性标注的分词方法
        seg_list = jieba.cut(sentence)
    else:
        # 进行词性标注的分词方法
        seg_list = psg.cut(sentence)
    return seg_list


# 去除干扰词
def word_filter(seg_list, pos=False):
    stopword_list = get_stopword_list()
    filter_list = []
    # 根据POS参数选择是否词性过滤
    ## 不进行词性过滤，则将词性都标记为n，表示全部保留
    for seg in seg_list:
        if not pos:
            word = seg
            flag = 'n'
        else:
            word = seg.word
            flag = seg.flag
        if not flag.startswith('n'):
            continue
        # 过滤停用词表中的词，以及长度为<2的词
        if not word in stopword_list and len(word) > 1:
            filter_list.append(word)

    return filter_list


# 数据加载，pos为是否词性标注的参数，corpus_path为数据集路径
def load_data(pos=False, corpus_path='/Users/mac/Desktop/Python自然语言处理实战/corpus.txt'):
    # 调用上面方式对数据集进行处理，处理后的每条数据仅保留非干扰词
    doc_list = []
    for line in open(corpus_path, 'r',encoding='utf-8'):
        content = line.strip()
        seg_list = seg_to_list(content, pos)
        filter_list = word_filter(seg_list, pos)
        doc_list.append(filter_list)

    return doc_list


# idf值统计方法
def train_idf(doc_list):
    idf_dic = {}
    # 总文档数
    tt_count = len(doc_list)

    # 每个词出现的文档数
    for doc in doc_list:
        for word in set(doc):
            idf_dic[word] = idf_dic.get(word, 0.0) + 1.0

    # 按公式转换为idf值，分母加1进行平滑处理
    for k, v in idf_dic.items():
        idf_dic[k] = math.log(tt_count / (1.0 + v))

    # 对于没有在字典中的词，默认其仅在一个文档出现，得到默认idf值
    default_idf = math.log(tt_count / (1.0))
    return idf_dic, default_idf


#  排序函数，用于topK关键词的按值排序
def cmp(e1, e2):
    import numpy as np
    res = np.sign(e1[1] - e2[1])
    if res != 0:
        return res
    else:
        a = e1[0] + e2[0]
        b = e2[0] + e1[0]
        if a > b:
            return 1
        elif a == b:
            return 0
        else:
            return -1

# TF-IDF类
class TfIdf(object):
    # 四个参数分别是：训练好的idf字典，默认idf值，处理后的待提取文本，关键词数量
    def __init__(self, idf_dic, default_idf, word_list, keyword_num):
        self.word_list = word_list
        self.idf_dic, self.default_idf = idf_dic, default_idf
        self.tf_dic = self.get_tf_dic()
        self.keyword_num = keyword_num

    # 统计tf值
    def get_tf_dic(self):
        tf_dic = {}
        for word in self.word_list:
            tf_dic[word] = tf_dic.get(word, 0.0) + 1.0

        tt_count = len(self.word_list)
        for k, v in tf_dic.items():
            tf_dic[k] = float(v) / tt_count

        return tf_dic

    # 按公式计算tf-idf
    def get_tfidf(self):
        tfidf_dic = {}
        for word in self.word_list:
            idf = self.idf_dic.get(word, self.default_idf)
            tf = self.tf_dic.get(word, 0)

            tfidf = tf * idf
            tfidf_dic[word] = tfidf

        tfidf_dic.items()
        # 根据tf-idf排序，去排名前keyword_num的词作为关键词
        for k, v in sorted(tfidf_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]:
            print(k + "/ ", end ='')
        print()


# 主题模型
class TopicModel(object):
    # 三个传入参数：处理后的数据集，关键词数量，具体模型（LSI、LDA），主题数量
    def __init__(self, doc_list, keyword_num, model='LSI', num_topics=4):
        # 使用gensim的接口，将文本转为向量化表示
        # 先构建词空间
        self.dictionary = corpora.Dictionary(doc_list)
        # 使用BOW模型向量化
        corpus = [self.dictionary.doc2bow(doc) for doc in doc_list]
        # 对每个词，根据tf-idf进行加权，得到加权后的向量表示
        self.tfidf_model = models.TfidfModel(corpus)
        self.corpus_tfidf = self.tfidf_model[corpus]

        self.keyword_num = keyword_num
        self.num_topics = num_topics
        # 选择加载的模型
        if model == 'LSI':
            self.model = self.train_lsi()
        else:
            self.model = self.train_lda()

        # 得到数据集的主题-词分布
        word_dic = self.word_dictionary(doc_list)
        self.wordtopic_dic = self.get_wordtopic(word_dic)

    def train_lsi(self):
        lsi = models.LsiModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)
        return lsi

    def train_lda(self):
        lda = models.LdaModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)
        return lda

    def get_wordtopic(self, word_dic):
        wordtopic_dic = {}

        for word in word_dic:
            single_list = [word]
            wordcorpus = self.tfidf_model[self.dictionary.doc2bow(single_list)]
            wordtopic = self.model[wordcorpus]
            wordtopic_dic[word] = wordtopic
        return wordtopic_dic

    # 计算词的分布和文档的分布的相似度，取相似度最高的keyword_num个词作为关键词
    def get_simword(self, word_list):
        sentcorpus = self.tfidf_model[self.dictionary.doc2bow(word_list)]
        senttopic = self.model[sentcorpus]

        # 余弦相似度计算
        def calsim(l1, l2):
            a, b, c = 0.0, 0.0, 0.0
            for t1, t2 in zip(l1, l2):
                x1 = t1[1]
                x2 = t2[1]
                a += x1 * x1
                b += x1 * x1
                c += x2 * x2
            sim = a / math.sqrt(b * c) if not (b * c) == 0.0 else 0.0
            return sim

        # 计算输入文本和每个词的主题分布相似度
        sim_dic = {}
        for k, v in self.wordtopic_dic.items():
            if k not in word_list:
                continue
            sim = calsim(v, senttopic)
            sim_dic[k] = sim

        for k, v in sorted(sim_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]:
            print(k + "/ ", end='')
        print()

    # 词空间构建方法和向量化方法，在没有gensim接口时的一般处理方法
    def word_dictionary(self, doc_list):
        dictionary = []
        for doc in doc_list:
            dictionary.extend(doc)

        dictionary = list(set(dictionary))

        return dictionary

    def doc2bowvec(self, word_list):
        vec_list = [1 if word in word_list else 0 for word in self.dictionary]
        return vec_list


def tfidf_extract(word_list, pos=False, keyword_num=10):
    doc_list = load_data(pos)
    idf_dic, default_idf = train_idf(doc_list)
    tfidf_model = TfIdf(idf_dic, default_idf, word_list, keyword_num)
    tfidf_model.get_tfidf()


def textrank_extract(text, pos=False, keyword_num=10):
    textrank = analyse.textrank
    keywords = textrank(text, keyword_num)
    # 输出抽取出的关键词
    for keyword in keywords:
        print(keyword + "/ ", end='')
    print()


def topic_extract(word_list, model, pos=False, keyword_num=10):
    doc_list = load_data(pos)
    topic_model = TopicModel(doc_list, keyword_num, model=model)
    topic_model.get_simword(word_list)


if __name__ == '__main__':
    text = '6月19日,《2012年度“中国爱心城市”公益活动新闻发布会》在京举行。' + \
           '中华社会救助基金会理事长许嘉璐到会讲话。基金会高级顾问朱发忠,全国老龄' + \
           '办副主任朱勇,民政部社会救助司助理巡视员周萍,中华社会救助基金会副理事长耿志远,' + \
           '重庆市民政局巡视员谭明政。晋江市人大常委会主任陈健倩,以及10余个省、市、自治区民政局' + \
           '领导及四十多家媒体参加了发布会。中华社会救助基金会秘书长时正新介绍本年度“中国爱心城' + \
           '市”公益活动将以“爱心城市宣传、孤老关爱救助项目及第二届中国爱心城市大会”为主要内容,重庆市' + \
           '、呼和浩特市、长沙市、太原市、蚌埠市、南昌市、汕头市、沧州市、晋江市及遵化市将会积极参加' + \
           '这一公益活动。中国雅虎副总编张银生和凤凰网城市频道总监赵耀分别以各自媒体优势介绍了活动' + \
           '的宣传方案。会上,中华社会救助基金会与“第二届中国爱心城市大会”承办方晋江市签约,许嘉璐理' + \
           '事长接受晋江市参与“百万孤老关爱行动”向国家重点扶贫地区捐赠的价值400万元的款物。晋江市人大' + \
           '常委会主任陈健倩介绍了大会的筹备情况。'

    pos = True
    seg_list = seg_to_list(text, pos)
    filter_list = word_filter(seg_list, pos)

    print('TF-IDF模型结果：')
    tfidf_extract(filter_list)
    print('TextRank模型结果：')
    textrank_extract(text)
    print('LSI模型结果：')
    topic_extract(filter_list, 'LSI', pos)
    print('LDA模型结果：')
    topic_extract(filter_list, 'LDA', pos)
-- 上述代码的输出结果
TF-IDF模型结果：
晋江市/ 城市/ 大会/ 爱心/ 中华/ 基金会/ 陈健倩/ 重庆市/ 许嘉璐/ 巡视员/ 
TextRank模型结果：
城市/ 爱心/ 救助/ 中国/ 社会/ 晋江市/ 基金会/ 大会/ 介绍/ 公益活动/ 
LSI模型结果：
中国/ 中华/ 爱心/ 基金会/ 项目/ 社会/ 城市/ 公益活动/ 全国/ 国家/ 
LDA模型结果：
晋江市/ 频道/ 大会/ 人大常委会/ 许嘉璐/ 陈健倩/ 巡视员/ 重庆市/ 爱心/ 民政局/
```
